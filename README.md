# ğŸ¦Ÿ Dengue Monitor

**Dengue Monitor** is a **data engineering and analytics** project focused on the epidemiological analysis of dengue cases in Brazil, using **real-scale data (hundreds of thousands of records)** and interactive dashboards.

The goal of the project is to demonstrate **technical capability**, **professional best practices**, and **data-driven decision making**, with a strong emphasis on **performance**, **data engineering**, and **analytical visualization**.

---

## ğŸ¯ Objective

The **Dengue Monitor** aims to consolidate large volumes of dengue notification data, apply **efficient database-level aggregations**, and expose **dashboards and analytical endpoints** for:

* Temporal analysis (epidemiological year)
* Geographic analysis (state and municipality)
* Demographic analysis (age range and gender)

The project was designed to:

* Scale to **hundreds of thousands of records**
* Minimize repeated database queries
* Demonstrate proficiency in **analytical SQL**, **data modeling**, **caching**, **performance optimization**, and **analytical APIs**

---

## ğŸ—ï¸ Overall Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSV / API  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Database (PostgreSQL)    â”‚
â”‚ - dengue_cases           â”‚
â”‚ - materialized views     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Data Layer        â”‚
â”‚ Repositories + Cache     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI    â”‚ â”‚   Dashboard (UI)     â”‚
â”‚ Analytics APIâ”‚ â”‚ Streamlit + Plotly   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§± Technology Stack

### Backend / Data

* **Python 3.12+**
* **SQLAlchemy**
* **Alembic** (migrations)
* **PostgreSQL** (analytical queries and aggregations)

### API

* **FastAPI**
* **Pydantic** (schemas)
* **Uvicorn**

The API exposes **analytical endpoints** consumed by the dashboard and allows future integration with other services.

### Visualization

* **Streamlit**
* **Plotly**

### Data Engineering

* Materialized Views
* Optimized indexes
* In-memory caching
* Clear separation between **Data Layer**, **API**, and **UI**

---

## ğŸ—‚ï¸ Project Structure

```text
dengue-monitor/
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py          # Streamlit app
â”‚   â””â”€â”€ utils.py        # UI helpers
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ lookups/
â”‚   â”‚   â”œâ”€â”€ loader.py
â”‚   â”‚   â”œâ”€â”€ municipios.json
â”‚   â”‚   â””â”€â”€ ufs.json
â”‚   â”‚
â”‚   â”œâ”€â”€ transformers/
â”‚   â”‚   â””â”€â”€ age.py
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis.py
â”‚   â”œâ”€â”€ enums.py
â”‚   â”œâ”€â”€ process_data.py
â”‚   â”‚
â”‚   â””â”€â”€ visualization/
â”‚       â”œâ”€â”€ matplotlib.py
â”‚       â”œâ”€â”€ plotly.py
â”‚       â””â”€â”€ seaborn.py
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â””â”€â”€ dengue_repository.py
â”‚   â”‚
â”‚   â”œâ”€â”€ database.py
â”‚   â””â”€â”€ models.py
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ location_service.py
â”‚   â”œâ”€â”€ routes.py
â”‚   â””â”€â”€ schemas.py
â”‚
â”œâ”€â”€ alembic/
â”‚   â””â”€â”€ versions/       # Migrations
â”‚
â”œâ”€â”€ main.py             # FastAPI entrypoint
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ—„ï¸ Database

### Main Table

**dengue_cases**

Stores raw dengue notification records.

Key columns:

* `nu_ano` â€” epidemiological year
* `sg_uf_not` â€” reporting state (UF)
* `id_municip` â€” municipality
* `idade` â€” patient age
* `cs_sexo` â€” gender (M, F, I)

### Materialized Views

The project relies on **materialized views** to avoid expensive real-time aggregations.

Example:

```sql
CREATE MATERIALIZED VIEW dengue_by_age_gender AS
SELECT
    sg_uf_not,
    nu_ano,
    FLOOR(idade / 10) * 10 AS faixa_inicio,
    cs_sexo,
    COUNT(*) AS casos
FROM dengue_cases
WHERE idade IS NOT NULL
GROUP BY sg_uf_not, nu_ano, faixa_inicio, cs_sexo;
```

âœ”ï¸ Fast queries
âœ”ï¸ Reduced database load
âœ”ï¸ Ideal for dashboards and analytical APIs

---

## âš¡ Performance & Best Practices

* Streamlit data caching
* Database-level aggregations
* Strategic indexing
* Avoids excessive database calls
* Clear separation between **UI**, **API**, and **Data Layer**

---

## ğŸ” Migrations (Alembic)

Adopted standard:

* `revision`: random ID generated by Alembic
* `down_revision`: explicit dependency reference
* Descriptive migration file names

Example:

```bash
alembic revision -m "create materialized view dengue_by_age_gender"
```

This approach ensures:

* Linear migration history
* Schema reproducibility
* Easy rollback

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\Activate.ps1 # Windows
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure environment

```bash
cp .env.example .env
```

Set the database environment variables.

### 4ï¸âƒ£ Run migrations

```bash
alembic upgrade head
```

### 5ï¸âƒ£ Download the dengue CSV data

The project expects **raw dengue CSV files** to be placed in the following directory:

```text
data/raw/
```

> This directory is intentionally excluded from version control.

You can obtain the CSV files using **one of the following options**:

---

### ğŸ”¹ Option 1 â€” Google Drive (recommended for quick setup)

A curated set of CSV files is available on Google Drive:

ğŸ‘‰ [https://drive.google.com/drive/folders/1GY_LRvW4pQ0isSVTN_LyWAji-ixbTdw6?usp=sharing](https://drive.google.com/drive/folders/1GY_LRvW4pQ0isSVTN_LyWAji-ixbTdw6?usp=sharing)

Steps:

1. Download one or more CSV files from the Drive folder
2. Create the directory if it does not exist:

   ```text
   data/raw/
   ```
3. Place the downloaded CSV files inside `data/raw/`

---

### ğŸ”¹ Option 2 â€” Official Brazilian Health Open Data Portal (DATASUS)

You can also download the data directly from the official source:

ğŸ‘‰ [https://dadosabertos.saude.gov.br/dataset/arboviroses-dengue](https://dadosabertos.saude.gov.br/dataset/arboviroses-dengue)

Steps:

1. Access the dataset page
2. Download the desired CSV files (by year or period)
3. Place the CSV files inside:

   ```text
   data/raw/
   ```

---

ğŸ“Œ **Notes**:

* The ingestion pipeline supports **multiple CSV files** inside `data/raw/`
* Files are processed sequentially
* Only the required columns are loaded into the database
* The data is normalized before insertion


### 6ï¸âƒ£ Process and load the data

From the **data/** directory:

```bash
python -m data.process_data
```

This script:

* Reads the raw CSV
* Applies sampling logic (100 records / month / UF)
* Normalizes fields
* Inserts data into PostgreSQL


### 6ï¸âƒ£.1ï¸âƒ£ Refresh Materialized Views (required)

After loading the data into PostgreSQL, you must refresh the materialized views used by the analytical queries and dashboards.

You can do this in one of the following ways:

---

### ğŸ”¹ Option 1 â€” Run refresh commands manually

Connect to your PostgreSQL database and execute:

```sql
REFRESH MATERIALIZED VIEW mv_top_municipios;
REFRESH MATERIALIZED VIEW mv_cases_heatmap_month_age;
REFRESH MATERIALIZED VIEW mv_cases_by_age_group;
REFRESH MATERIALIZED VIEW mv_cases_by_gender_age_group;
```

---

### ğŸ”¹ Option 2 â€” Run the provided SQL script (recommended)

The project includes a helper script that refreshes all materialized views at once.

From the project root, run:

```bash
psql -d <your_database_name> -f scripts/database/refresh_materialized_views.sql
```

---

ğŸ“Œ **Notes**:

* This step is required every time new data is loaded
* Materialized views significantly improve dashboard performance
* The script is safe to run multiple times


### 7ï¸âƒ£ Start the FastAPI server

```bash
uvicorn main:app --reload
```

The API exposes analytical endpoints consumed by the dashboard.

### 8ï¸âƒ£ Start the dashboard

```bash
streamlit run dashboard/app.py
```

---

## ğŸ¤ Contribution

Contributions are welcome!

1. Fork the project
2. Create a branch (`feat/my-feature`)
3. Commit following **Conventional Commits**
4. Open a Pull Request

---

## ğŸ“„ License

This project is distributed under the MIT License.

---

## ğŸ‘¨â€ğŸ’» Author

Project developed by **Jefferson** as an **advanced study and technical portfolio** in:

* Data Engineering
* Epidemiological Analysis
* Analytical Visualization
